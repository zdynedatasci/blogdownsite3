[{"authors":["admin"],"categories":null,"content":"I am a dedicated and creative May 2020 graduate of Willamette University’s MBA and Graduate Certificate in Data Science programs. My experience and professional interests, beginning as a 15-year-old volunteer public health research assistant, intersect data science and quantitative research programming with public policy, public administration, and non-profit management. After graduation, I plan to enter a career using data science principles to help solve public administration and policy problems. My experience in data science, project management, and public health research, along with my academic background in mixed-methods social sciences, prepares me well to join a team solving management and policy problems. Through my experience and education, I have developed the following skills:\n Data Analysis Tools: R, Tableau, Excel, SQL, SAS, Python, SPSS, jamovi Statistics Financial and Managerial Accounting Project Management and Program Facilitation Tutoring and Tutorial Curation Public Speaking  ","date":1554595200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1554595200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/zachary-dyne/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zachary-dyne/","section":"authors","summary":"I am a dedicated and creative May 2020 graduate of Willamette University’s MBA and Graduate Certificate in Data Science programs. My experience and professional interests, beginning as a 15-year-old volunteer public health research assistant, intersect data science and quantitative research programming with public policy, public administration, and non-profit management.","tags":null,"title":"Zachary Dyne","type":"authors"},{"authors":["Zachary Dyne"],"categories":null,"content":"I am a dedicated and creative May 2020 graduate of Willamette University’s MBA and Graduate Certificate in Data Science programs. My experience and professional interests, beginning as a 15-year-old volunteer public health research assistant, intersect data science and quantitative research programming with public policy, public administration, and non-profit management. After graduation, I plan to enter a career using data science principles to help solve public administration and policy problems.\nAfter three years of non-profit program management and curriculum development, I’ve enjoyed applying my data science skills to my passion for community improvement through internships, consulting projects, and peer tutoring. During my data analytics internship focused at United Way of the Mid-Willamette Valley, I used Tableau and R for strategic planning, resource development, and fundraising analytics projects. I carried this experience of making end-user focused analytics products to my work at the Willamette MBA Community Grant Program, were I served as the Project Manager and Lead Financial Data Analyst. In addition to improving my leadership, program evaluation, and financial analysis skills, I built the program’s first SQL database to organize impact metrics on previously funded projects. I view data science as a useful set of problem-solving tools, and I take pride in being able to deploy them on teams with a range of technical familiarity.\nAside from data science and analytics, I consider communication and collaboration to be two of my greatest strengths. Additionally, from interviewing emergency room patients in Spanish at a county hospital, guiding litigants through filing paperwork for self-representation in civil court at a non-profit law firm, and co-administering a service-learning trip with five Japanese exchange students, I’ve had many opportunities to work with people from a range of cultural and linguistic backgrounds. My experience in data science, project management, and public health research, along with my academic background in mixed-methods social sciences, prepares me well to join a team solving management and policy problems.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"d8f258c323db746988131e8c2d192f9a","permalink":"/author/zachary-dyne/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zachary-dyne/","section":"authors","summary":"I am a dedicated and creative May 2020 graduate of Willamette University’s MBA and Graduate Certificate in Data Science programs. My experience and professional interests, beginning as a 15-year-old volunteer public health research assistant, intersect data science and quantitative research programming with public policy, public administration, and non-profit management.","tags":null,"title":"Zachary Dyne","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":[],"categories":[],"content":"\r\rThe lack of access to affordable housing is one of the biggest social issues throughout the United States. This is also true locally in Oregon, as the Oregon Center for Public Policy highlights. Both for-profit and non-profit organizations have become involved in building and managing affordable housing units, but there is clearly more work that needs to be done. During my time at United Way of the Mid-Willamette Valley, the grantmaking organization had added a goal of becoming an affordable housing provider, or at minimum become a catalyst for other organizations to take on serious affordable housing efforts.\nI was curious to see if I could find any data on affordable housing projects throughout Oregon. Where are they located? Where is the greatest need? How do the locations and needs overlap? Do the locations and need sufficiently overlap? Of course, the answer to the final question will not be “yes” until homelessness is eradicated. However, I wanted to see if I could get a glimpse at the current state of affairs.\nThe Data: RSocrata, tidycensus, and leaflet\nFortunately, there is a plethora of R packages that make identifying, searching for, analyzing, and visualizing data to answer get at these questions possible! I will principally make use of RSocrata, tidycensus, and leaflet.\n\rRSocrata allows you to pull data from a socrata open data portal, which is how many entities publish open datasets, directly into R. The State of Oregon published a dataset with a list of affordable housing properties throughout the State, including street addresses and its management company.\n\rThe tidycensus package allows you to pull data directly from the US Census Bureau into R. The Census’s American Community Survey (ACS) includes a question “Gross Rent as a Percentage of Household Income in the Past 12 Months” (identified as Table B25070). This question could be used to estimate how many people experience housing cost burden, which the US Department of Housing and Urban Development (HUD) defines as spending at or more than 30% of household income on housing. Other entities define this figure at 50%, but I will use the 30% threshold for this analysis. It’s important to remember that the ACS does not poll as many people as does the 10-year full census, which makes for higher margins of error in these estimates. With fewer people polled, the estimates have a higher chance of being incorrect. While they probably shouldn’t be used for publication in The Quarterly Journal of Economics, they can still be useful for our goal to get a sense of things.\n\rThe leaflet package provides map-building capability in R. We can overlay these two data sources on a single map using leaflet to visualize percent of population who experiences a housing cost burden by county and the location of the listed affordable housing units.\n\r\rOf course, I’ll use several other R packages to tie these unique features together, including tidyverse, ggmap, and tigris to name a few.\n# load packages\rlibrary(tidyverse) # for all things data science\rlibrary(RSocrata) # to access Oregon\u0026#39;s Affordable Housing Index via the Socrata API\rlibrary(tidycensus)# to access US Census data via Census API\rlibrary(kableExtra)# for HTML table formatting\rlibrary(scales) # for formatting percents\rlibrary(widgetframe)\rlibrary(htmlwidgets)\rlibrary(htmltools)\r#library(ggmap)\r# read Housing Index Data from State of Oregon\u0026#39;s Socrata data portal\r# https://data.oregon.gov/Health-Human-Services/Affordable-Housing-Inventory/bq26-qyg4\rHousingIndex \u0026lt;- read.socrata(\u0026quot;https://data.oregon.gov/resource/bq26-qyg4.csv\u0026quot;)\r# read and wrangle ACS data (each table and then bind)\rTotal \u0026lt;- get_acs(geography = \u0026quot;county\u0026quot;, variables = \u0026quot;B25070_001\u0026quot;, state = \u0026quot;OR\u0026quot;, year = 2018) %\u0026gt;%\rmutate(v2 = \u0026quot;Total\u0026quot;)\rb1 \u0026lt;- get_acs(geography = \u0026quot;county\u0026quot;, variables = \u0026quot;B25070_002\u0026quot;, state = \u0026quot;OR\u0026quot;, year = 2018) %\u0026gt;%\rmutate(v2 = \u0026quot;e\u0026gt;10%\u0026quot;)\rb2 \u0026lt;- get_acs(geography = \u0026quot;county\u0026quot;, variables = \u0026quot;B25070_003\u0026quot;, state = \u0026quot;OR\u0026quot;, year = 2018) %\u0026gt;%\rmutate(v2 = \u0026quot;e10%-14.9%\u0026quot;)\rb3 \u0026lt;- get_acs(geography = \u0026quot;county\u0026quot;, variables = \u0026quot;B25070_004\u0026quot;, state = \u0026quot;OR\u0026quot;, year = 2018) %\u0026gt;%\rmutate(v2 = \u0026quot;e15%-19.9%\u0026quot;)\rb4 \u0026lt;- get_acs(geography = \u0026quot;county\u0026quot;, variables = \u0026quot;B25070_005\u0026quot;, state = \u0026quot;OR\u0026quot;, year = 2018) %\u0026gt;%\rmutate(v2 = \u0026quot;e20%-24.9%\u0026quot;)\rb5 \u0026lt;- get_acs(geography = \u0026quot;county\u0026quot;, variables = \u0026quot;B25070_006\u0026quot;, state = \u0026quot;OR\u0026quot;, year = 2018) %\u0026gt;%\rmutate(v2 = \u0026quot;e25%-29.9%\u0026quot;)\rb6 \u0026lt;- get_acs(geography = \u0026quot;county\u0026quot;, variables = \u0026quot;B25070_007\u0026quot;, state = \u0026quot;OR\u0026quot;, year = 2018) %\u0026gt;%\rmutate(v2 = \u0026quot;e30%-34.9%\u0026quot;)\rb7 \u0026lt;- get_acs(geography = \u0026quot;county\u0026quot;, variables = \u0026quot;B25070_008\u0026quot;, state = \u0026quot;OR\u0026quot;, year = 2018) %\u0026gt;%\rmutate(v2 = \u0026quot;e35%-39.9%\u0026quot;)\rb8 \u0026lt;- get_acs(geography = \u0026quot;county\u0026quot;, variables = \u0026quot;B25070_009\u0026quot;, state = \u0026quot;OR\u0026quot;, year = 2018) %\u0026gt;%\rmutate(v2 = \u0026quot;e40%-49.9%\u0026quot;)\rb9 \u0026lt;- get_acs(geography = \u0026quot;county\u0026quot;, variables = \u0026quot;B25070_010\u0026quot;, state = \u0026quot;OR\u0026quot;, year = 2018) %\u0026gt;%\rmutate(v2 = \u0026quot;e\u0026gt;=50%\u0026quot;)\rRentPercent \u0026lt;- bind_rows(b1, b2, b3, b4, b5, b6, b7, b8, b9) # bind rows of separate ACS tables\rRentBurden \u0026lt;- bind_rows(b6, b7, b8, b9) # bind rows only for rent burden bins (e\u0026gt;= 30%)\rThe ACS organizes each question table into sub-tables by bin (“Less than 10%”, “10%-14.9%”, etc). Using tidycensus to create a single dataframe of the table with each bin present requires pulling each sub-table separately, creating a sub-table identifier (which I called v2), and binding the rows together. Here is a glimpse of how the data looks after all of that.\nhead(RentBurden) %\u0026gt;%\rkable() %\u0026gt;%\rkable_styling(\u0026quot;striped\u0026quot;)\r\r\rGEOID\r\rNAME\r\rvariable\r\restimate\r\rmoe\r\rv2\r\r\r\r\r\r41001\r\rBaker County, Oregon\r\rB25070_007\r\r180\r\r99\r\re30%-34.9%\r\r\r\r41003\r\rBenton County, Oregon\r\rB25070_007\r\r994\r\r246\r\re30%-34.9%\r\r\r\r41005\r\rClackamas County, Oregon\r\rB25070_007\r\r3991\r\r384\r\re30%-34.9%\r\r\r\r41007\r\rClatsop County, Oregon\r\rB25070_007\r\r566\r\r161\r\re30%-34.9%\r\r\r\r41009\r\rColumbia County, Oregon\r\rB25070_007\r\r642\r\r174\r\re30%-34.9%\r\r\r\r41011\r\rCoos County, Oregon\r\rB25070_007\r\r860\r\r199\r\re30%-34.9%\r\r\r\r\rWe can use the tidyverse functions to find the estimated total housing burdened population, total population, and percent of population experiencing housing a cost burden by county. This is similar to creating a pivot table in Excel.\nRentBurdenPercent \u0026lt;- RentBurden %\u0026gt;% # find total number and percent rent burdened\rgroup_by(NAME) %\u0026gt;%\rsummarise(TotalRentBurden = sum(estimate)) %\u0026gt;%\rmerge(Total, by = \u0026quot;NAME\u0026quot;) %\u0026gt;%\rselect(NAME, GEOID, TotalRentBurden, estimate) %\u0026gt;%\rrename(\u0026quot;Total\u0026quot; = \u0026quot;estimate\u0026quot;) %\u0026gt;%\rmutate(\u0026quot;PercentRentBurden\u0026quot; = TotalRentBurden/Total) %\u0026gt;%\rmutate(\u0026quot;PercentBurdenFull\u0026quot; = PercentRentBurden*100) %\u0026gt;%\rarrange(desc(PercentRentBurden))\rRentBurdenPercent %\u0026gt;% # display nicely select(-GEOID, -PercentBurdenFull) %\u0026gt;%\rmutate(PercentRentBurden = percent(PercentRentBurden)) %\u0026gt;%\rrename(\u0026quot;County\u0026quot; = \u0026quot;NAME\u0026quot;, \u0026quot;Estimated Housing Burdened Population\u0026quot; = \u0026quot;TotalRentBurden\u0026quot;,\r\u0026quot;Estimated Total Population\u0026quot; = \u0026quot;Total\u0026quot;,\r\u0026quot;Perect Hoousing Burdened\u0026quot; = \u0026quot;PercentRentBurden\u0026quot;) %\u0026gt;%\rkable() %\u0026gt;%\rkable_styling(\u0026quot;striped\u0026quot;) %\u0026gt;%\rscroll_box(width = \u0026quot;100%\u0026quot;, height = \u0026quot;300px\u0026quot;)\r\r\rCounty\r\rEstimated Housing Burdened Population\r\rEstimated Total Population\r\rPerect Hoousing Burdened\r\r\r\r\r\rBenton County, Oregon\r\r8228\r\r15134\r\r54.368%\r\r\r\rJosephine County, Oregon\r\r6478\r\r11957\r\r54.177%\r\r\r\rCurry County, Oregon\r\r1703\r\r3147\r\r54.115%\r\r\r\rLane County, Oregon\r\r33074\r\r62345\r\r53.050%\r\r\r\rJackson County, Oregon\r\r17066\r\r32301\r\r52.834%\r\r\r\rYamhill County, Oregon\r\r5744\r\r11029\r\r52.081%\r\r\r\rColumbia County, Oregon\r\r2520\r\r4983\r\r50.572%\r\r\r\rMultnomah County, Oregon\r\r74513\r\r147547\r\r50.501%\r\r\r\rPolk County, Oregon\r\r5112\r\r10277\r\r49.742%\r\r\r\rTillamook County, Oregon\r\r1661\r\r3344\r\r49.671%\r\r\r\rDeschutes County, Oregon\r\r12311\r\r24865\r\r49.511%\r\r\r\rLinn County, Oregon\r\r8343\r\r17120\r\r48.732%\r\r\r\rMarion County, Oregon\r\r22804\r\r47366\r\r48.144%\r\r\r\rKlamath County, Oregon\r\r4714\r\r9918\r\r47.530%\r\r\r\rClackamas County, Oregon\r\r21524\r\r45810\r\r46.985%\r\r\r\rWashington County, Oregon\r\r38659\r\r84148\r\r45.942%\r\r\r\rCoos County, Oregon\r\r4192\r\r9379\r\r44.696%\r\r\r\rLincoln County, Oregon\r\r3345\r\r7496\r\r44.624%\r\r\r\rDouglas County, Oregon\r\r6200\r\r14272\r\r43.442%\r\r\r\rClatsop County, Oregon\r\r2612\r\r6092\r\r42.876%\r\r\r\rMalheur County, Oregon\r\r1729\r\r4164\r\r41.523%\r\r\r\rWallowa County, Oregon\r\r443\r\r1087\r\r40.754%\r\r\r\rJefferson County, Oregon\r\r972\r\r2404\r\r40.433%\r\r\r\rCrook County, Oregon\r\r1087\r\r2735\r\r39.744%\r\r\r\rLake County, Oregon\r\r489\r\r1266\r\r38.626%\r\r\r\rUnion County, Oregon\r\r1423\r\r3767\r\r37.775%\r\r\r\rUmatilla County, Oregon\r\r3672\r\r9748\r\r37.669%\r\r\r\rBaker County, Oregon\r\r782\r\r2077\r\r37.650%\r\r\r\rWasco County, Oregon\r\r1409\r\r3866\r\r36.446%\r\r\r\rSherman County, Oregon\r\r94\r\r260\r\r36.154%\r\r\r\rHood River County, Oregon\r\r992\r\r2901\r\r34.195%\r\r\r\rGrant County, Oregon\r\r323\r\r977\r\r33.060%\r\r\r\rMorrow County, Oregon\r\r380\r\r1213\r\r31.327%\r\r\r\rHarney County, Oregon\r\r244\r\r830\r\r29.398%\r\r\r\rGilliam County, Oregon\r\r90\r\r311\r\r28.939%\r\r\r\rWheeler County, Oregon\r\r36\r\r176\r\r20.455%\r\r\r\r\r\rThe raw percent figures are pretty incredible to look at. Eight of Oregon’s 36 counties have an estimated housing-burdened population of over half of their total population! Many other counties’ estimates are just below 50% as well. Even Wheeler County’s estimated 20.5% seems very high.\nMap\nThe goal is to create a map of the percent housing burdened by county and add each of the affordable housing complexes listed by the State.\nWe first have to find the latitude and longitude coordinates (AKA geocode) the complex’s street addresses in order to plot them on a map.\n#detach(\u0026quot;package:kableExtra\u0026quot;, unload = TRUE)\r#detach(\u0026quot;package:RSocrata\u0026quot;, unload = TRUE)\rlibrary(leaflet)\rlibrary(tigris)\r#options(tigris_use_cache = TRUE)\rlibrary(widgetframe)\rorigAddress \u0026lt;- HousingIndex %\u0026gt;%\rmutate(addresses = paste0(address, city, \u0026quot;Oregon\u0026quot;, sep = \u0026quot;, \u0026quot;)) %\u0026gt;%\rselect(addresses) %\u0026gt;%\rdata.frame(stringsAsFactors = FALSE)\r###### This geocoding code chunk requires a lot of processing power and took seven minutes per run time. ###### To accomodate, I ran it once with the line below, exported to .csv and reloaded it below. # geocode Housing Index using ggmap (which requires a Google API as of late 2018)\r#library(ggmap)\r# GeocodedHousingIndex \u0026lt;- geocode(origAddress$addresses, output = \u0026quot;latlona\u0026quot;, source = \u0026quot;google\u0026quot;)\rGeocodedHousingIndex \u0026lt;- read_csv(\u0026quot;C:/Users/Lenovo/Desktop/blogdown site bside posts/GeocodedHousingIndex.csv\u0026quot;) %\u0026gt;%\rsf::st_as_sf(coords = c(\u0026quot;lon\u0026quot;, \u0026quot;lat\u0026quot;), crs = \u0026#39;+proj=longlat +datum=WGS84\u0026#39;)\rMapping with leaflet\nlibrary(sf)\rcounties \u0026lt;- counties(\u0026quot;Oregon\u0026quot;, cb = TRUE, progress_bar = FALSE, class = \u0026quot;sf\u0026quot;) # fetch county shapefile from US Census Bureau\rcounties_merged_acs \u0026lt;- merge(counties, RentBurdenPercent, by.x = \u0026quot;GEOID\u0026quot;, by.y = \u0026quot;GEOID\u0026quot;) # geo_join() with RentBurdenPrecent data\rPercentBurdenPal \u0026lt;- colorNumeric(palette = \u0026quot;Greens\u0026quot;, domain=counties_merged_acs$PercentRentBurden) # create numeric color palet based on the range of PercenRentBurden\r# create content for popups to appear each county\rCountyPopupContent \u0026lt;- paste0(\u0026quot;\u0026lt;b\u0026gt;\u0026quot;, counties_merged_acs$NAME.1, \u0026quot;\u0026lt;/b\u0026gt;\u0026quot;, \u0026quot;\u0026lt;br\u0026gt;\u0026quot;,\r\u0026quot;Estimated Percent Rent Burdened: \u0026quot;, percent(counties_merged_acs$PercentRentBurden), \u0026quot;\u0026lt;br\u0026gt;\u0026quot;,\r\u0026quot;Estimated Count of Rent Burdened: \u0026quot;, counties_merged_acs$TotalRentBurden, \u0026quot;\u0026lt;br\u0026gt;\u0026quot;,\r\u0026quot;Estimated Total Population: \u0026quot;, counties_merged_acs$Total)\r# create content for popups to appear each housing complex\rHousingPopupContent \u0026lt;- paste(\u0026quot;\u0026lt;b\u0026gt;\u0026quot;, HousingIndex$project_name, \u0026quot;\u0026lt;/b\u0026gt;\u0026quot;, \u0026quot;\u0026lt;br\u0026gt;\u0026quot;,\r\u0026quot;Address: \u0026quot;, HousingIndex$address,\u0026quot;, \u0026quot;, HousingIndex$city, \u0026quot;, Oregon\u0026quot;, \u0026quot;\u0026lt;br\u0026gt;\u0026quot;,\r\u0026quot;Management: \u0026quot;, HousingIndex$management, \u0026quot;\u0026lt;br\u0026gt;\u0026quot;,\r\u0026quot;Total Units: \u0026quot;, HousingIndex$total_units)\rthis.map \u0026lt;- counties_merged_acs %\u0026gt;%\rsf::st_transform(crs = \u0026#39;+proj=longlat +datum=WGS84\u0026#39;) %\u0026gt;%\rleaflet() %\u0026gt;% # create leaflet of counties_merged_acs data defined above\raddTiles() %\u0026gt;% # add tiles to map\raddMarkers(data = GeocodedHousingIndex, # add makers for each housing complex\rpopup = HousingPopupContent, # insert HousingPopupContent\rclusterOptions = markerClusterOptions()) %\u0026gt;% # cluster icons (w/o this feature the map is too crowded to read)\raddPolygons(fillColor = PercentBurdenPal(counties_merged_acs$PercentRentBurden), # add polygonns (shape of OR\u0026#39;s counties)\rfillOpacity = 0.80, # set opacity\rhighlightOptions = highlightOptions(color = \u0026quot;white\u0026quot;, weight = 2, bringToFront = TRUE), # set highlight feature col = \u0026quot;#302E2D\u0026quot;, # set color for county boarder (defaults to a Dodger blue as if to remind you to change it)\rweight = 1, # set boarder weight\rpopup = ~CountyPopupContent) %\u0026gt;% # insert CountyPopupContent addLegend(pal = PercentBurdenPal, # define color palet for legand\rvalues = counties_merged_acs$PercentRentBurden, # define legend valees\rtitle = paste(\u0026quot;% Housing\u0026quot;, \u0026quot;\u0026lt;br\u0026gt;\u0026quot;, \u0026quot;Burdened\u0026quot;), # define legend title\rposition = \u0026quot;bottomright\u0026quot;) %\u0026gt;% #define legend positin\rsetView(lat= 44.000000, lng=-120.500000, zoom=6.45) # set lat, long, and zoom to view Oregon\r\rThe marker points for the housing complexes are clustered so that they are visable. The map would be very difficult to read without this option turned on.\nFinal Thoughts\nKind of a neat map. Somewhat unsprisingly, the amount of affordable housing complexes in each area is generally associated with population density. This also highlights the need for and logistical complexity of affordable housing in rural areas as well.\nUseful project resources\nI read a number of useful resources in creating this map. A few of the most useful are listed below. Additionally, a tromedous “thank you” to Prof. Robert Walker for helping me with some unusal debugging issues on this project as well as for introducing me to R in the first place.\nLeaflet Mapping:\n\rLeaflet for R documentation\rProf. Aaron Maxwell at West Virginia University\rProf. Benjamin Soltoff at the University of Chicago\r\rGIS data wrangling and transforming:\n\rAndrew Ba Tran of the Washington Post in his book R-Journalism.\rAleszu Bajak at StoryBench.\r\r","date":1591302276,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591302276,"objectID":"e94b756ab9edc906f00aa6279b593720","permalink":"/post/2020-06-04-affordable-housing-map/","publishdate":"2020-06-04T13:24:36-07:00","relpermalink":"/post/2020-06-04-affordable-housing-map/","section":"post","summary":"I created an interactive map displaying housing burden rates and affordable housing complexes in Oregon.","tags":[],"title":"Affordable Housing Map","type":"post"},{"authors":[],"categories":["SQL","Data Engineering","MBA"],"content":"One of my favorite courses I took during my time at Atkinson was GSMDS5003: Data Engineering, taught by the one and only Prof. Henry Bi. We not only learned how to write SQL code but also about the fundamentals of database design, including entity relational (ER) modeling, standardization, and normalization. Learning the structure of well designed databases made learning to retrieve data via SQL a fairly straitforward extension. The course\u0026rsquo;s final project was to create a database of our own using Microsoft Access, so I decided to use this opportunity to solve a different problem I was dealing with: the information scattered throughout the Google Drive of the Willamette MBA Community Grant Program.\nWillamette MBA Community Grant Program The Willamette MBA Community Grant Program is the only 100% student-administered grantmaking program in the US and provides funding opportunities for nonprofit or social service organizations serving Oregon\u0026rsquo;s Marion, Polk, and/or Yamhill counties. As students facilitating multiple rounds of grantee applications by reviewing organizational and operational metrics while following up on the results of projects from previous funding cycles, it was getting difficult to keep track of all of the information we needed. As the project manager and lead financial data analyst, the disorganization made creating tools for internal communication, financial statement analysis, and risk modeling particularly tedious. Additionally, because it is completely student-run, the program experiences 100% annual turnover, with the exception of the two faculty instructors. This makes organizing institutional knowledge all the more difficult yet essential. Seems like a problem an Access SQL database could solve!\nDatabase Overview My goal in creating this database was to make it as flexable as possible to adapt to future itrations of the program. The Grant Program only just finished its fourth year and there are many things that could hypothetically happen that haven\u0026rsquo;t yet. As such, the database seems a bit redundant (in SQL-talk: it has more many-to-many relationships than initally seems necessary). For example, an EnrollIn relationship needs its own table since it is hypothetically possible for a student to enroll in the course more than once (they coud be a JD/MBA or an MBA-for-Life student). If we just assumed that each student will enroll in the course only once, EnrollIn could just be a column in the Student table. Feel free to check out the full project description for all of the database requirements.\nWhile I can\u0026rsquo;t share the content of many tables due to privacy reasons, I can share the database\u0026rsquo;s relationship diagrams as a proof of concept.\nEntity Relation Model Access Relational Diagram Hey, it works! Sample problem: List the organization name, project name, description, and amount received for each project operated by an organization who has operated more than one project. Provide the list in descending order by amount.\nFinal Thoughts I had a lot of fun working on this project since it combines two of my primary interests: data science and nonprofit management. Thank you to Prof. Henry Bi for your guidance and encouragement along the way! If you\u0026rsquo;re interested to learn more about the Willamette MBA Community Grant Program, check out the recently published 2019-2020 Annual Report!\n","date":1590192000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590254924,"objectID":"f478d6527f924f30ed36cee04b3a4903","permalink":"/post/agsmgrantsql/agsmgrantssql/","publishdate":"2020-05-23T00:00:00Z","relpermalink":"/post/agsmgrantsql/agsmgrantssql/","section":"post","summary":"This post provides an overview of the SQL database I created for the Willamette MBA Community Grant Program during my MBA.","tags":["SQL","Access","MBA","Willamette MBA Community Grant Program"],"title":"An Overview of the Willamette MBA Community Grant Program SQL Database","type":"post"},{"authors":[],"categories":"MBA","content":"Economic mobility, the ability for individuals to economically fare better than their parents did, is an essential component for the continued livelihood of a city. This is especially true for Kansas City in its efforts to revitalize what was once a lively economic hub based on manufacturing and transportation to a modern metropolis fueled by technology and innovation. Recent efforts in economic development from city and state administered programs, such as the Opportunity Zone Program, the KC Rise co-investment fund, and the BioNexus KC life science development initiative, have contributed to a 5.7% growth in real GDP and a 7.8% growth in median household income from 2013 to 2017. While these metrics are certainly a positive indication for the city’s future, Kansas City continues to lag behind its self-identified peer cities on both metrics. Additionally, not all the development has benefited all Kansas City residents equally. Several neighborhoods, particularly those on the east side of the city, continue to experience the effects of decades of disinvestment, including underfunded public schools resulting from uneven property taxes. According to the Stanford Education Opportunity Monitoring Project, non-poor students in the Kansas City School District perform at an average of 0.12 grades above grade level, whereas poor students perform at an average of 1.85 grades below grade level (Exhibit 1).\nExhibit 1 The red dot represents data for the Kansas City School District\nAccess to quality K-12 education is known to have a strong relationship with earnings later in life. Research by Harvard economist Raj Chetty found this to be true even after controlling for intergenerational wealth. Chetty’s lab financed families in Seattle, Washington to move from Section 8 housing to areas with better public schools, which rarely had Section 8 housing, and the children in those families saw a large increase in their earnings compared to controls. While imitating Chetty’s methods appears to be a good starting point, it is not financially feasible at scale. To improve economic mobility, the City of Kansas City must create policies and programs that provide greater access to higher quality K-12 education for students from low-income families. While school vouchers or busing programs seem to be effective for a smaller number of students in the short run, a more elegant tactic possibly with further-reaching impacts involves adjusting Kansas City’s residential zoning laws.\nBecause public schools are funded primarily through property taxes, those who live in areas with less valuable residential property are at a structural disadvantage in opportunities for public K-12 education. Moreover, because much of Kansas City’s residential areas are zoned R-80, meaning that the minimum lot size is 80,000 square feet, there are less opportunities to develop new homes in areas with higher valued real estate. This zoning law also keeps the price of each individual lot higher, which further increase economic segregation across the city and reinforces the difference in education and lifetime earnings outcomes between poor nonpoor children.\nAccordingly, to increase opportunities for economic mobility, the City of Kansas City should consider amending the residential zoning codes to allow for smaller single-family developments across the city. This will make moving to areas with higher valued land and higher funded public schools more accessible to lower-income families. It works towards incentivizing similar moving patters to those Chetty’s lab privately financed without requiring the city to either raise local taxes or routinely pull funding from elsewhere in the city’s annual budget to gather adequate public funding.\nShrinking the minimum residential lot size would also have the positive effect of making buying a home more affordable across the city. Despite buying a home being 66% less expensive than renting a home in Kansas City, home ownership has declined across the city from 73% in 2007 to just 62% in 2016. This discrepancy results from an inability to afford an initial down payment on a home, despite it being a cost-saving long-term investment. If minimum lot sizes were lower, causing home prices to decline, a down payment on a home becomes more accessible to more families. Greater access to home ownership will also contributes to long-term economic mobility. Amending the zoning code is one tactic that the City of Kanas City should employ to increase opportunities for economic mobility through increased access to education and home ownership.\n","date":1578873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590604860,"objectID":"a872509dc454651947dbee627da4bb75","permalink":"/post/zoninglawskcmo/kcmo-impact-zoning-laws-on-education/","publishdate":"2020-01-13T00:00:00Z","relpermalink":"/post/zoninglawskcmo/kcmo-impact-zoning-laws-on-education/","section":"post","summary":"This is a policy memo I wrote about the impact of zoning laws on education outcomes and economic mobility in Kansas City, Missouri.","tags":["Public Policy","Real Estate","Education"],"title":"Relocation, Education, and Incentivization: Adjusting residential zoning laws to improve economic mobility in Kansas City, Missouri","type":"post"},{"authors":[],"categories":["R","Healthcare","Medicare","Medicaid"],"content":"\r\rIntroduction\rProject Overview: Medicare Hospice Care Expenditures\rData Preparation:\rMerge with “Region and Division” dataset\r\rExploratory Data Analysis and Regression Modeling\rOutcome Variable: Medicare Payments\rPredictor Variables: Data Exploration and Liner Models by Variable Type\rNumeric Predictors of Interest\rFinal Conclusions and Takeaways\r\r\r\rIntroduction\rProject Overview: Medicare Hospice Care Expenditures\rThis project explores the nature of Medicare hospice care expenditures across the United States. I found these datasets on Kaggle, uploaded by the Center for Medicare and Medicaid Research.\nWhat is Hospice Care?\r\rHospice care, otherwise known as end-of-life care, is a type of care typically used by patients who feel their treatment of a terminal illness is not helping them improve.\rUsing hospice is not a signal of “giving up”. Hospice care focuses on quaility of life rather than directly treating the illness.\r\rIt focuses on “relief from pain, shortness of breath, and other symptoms so that you can focus on the people and things you care about the most” (WebMD).\r\r\r\rResearch Questions\r\rWhat factors predict Medicare payments to providers by diagnosis, length of stay, site of service, hours per week, and number of beneficiaries?\rWith three years of data, I aim to learn how these factors change overtime by themselves while also seeing whether their interactions do as well.\rWhat is considered “typical” for payments to hospice care providers? What causes atypical payments to providers?\r\r\r\rData Preparation:\r#load required packages\rlibrary(tidyverse)\rlibrary(ggthemes)\rlibrary(ggplot2); theme_set(theme_tufte())\rlibrary(data.table)\rlibrary(GGally)\rlibrary(cowplot)\rlibrary(mosaic)\rlibrary(scales)\rlibrary(broom)\rlibrary(purrr)\rlibrary(moderndive)\r#read datsets for each year, add a \u0026#39;year\u0026#39; factor variable to each dataset, and bind together\rsetwd(\u0026quot;C:/Users/Lenovo/Desktop/AGSM/Fall2019/RCertificate/Medicare Data/\u0026quot;)\rProvider2016 \u0026lt;- read_csv(\u0026quot;medicare-hospice-use-and-spending-by-provider-aggregate-report-cy-2016.csv\u0026quot;)\rProvider2016$Year \u0026lt;- as.factor(2016)\rProvider2015 \u0026lt;- read_csv(\u0026quot;medicare-hospice-use-and-spending-by-provider-aggregate-report-cy-2015.csv\u0026quot;)\rProvider2015$Year \u0026lt;- as.factor(2015)\rProvider2014 \u0026lt;- read_csv(\u0026quot;medicare-hospice-use-and-spending-by-provider-aggregate-report-cy-2014.csv\u0026quot;)\rProvider2014$Year \u0026lt;- as.factor(2014)\rD \u0026lt;- rbind(Provider2016, Provider2015)\rD \u0026lt;- rbind(D, setnames(Provider2014, names(D)))\r\rMerge with “Region and Division” dataset\rThe original datasets do not include region, which may be a more useful metric than analyzing by state. I found this dataset on Kaggle, and it includes state names, state codes, region, and division. Classifications are as determined by the US Census Bureau. Let’s merge it with the Medicare master dataset to be able to analyze payments by region.\nstates \u0026lt;- read_csv(\u0026quot;C:/Users/Lenovo/Desktop/AGSM/Fall2019/RCertificate/Medicare Data/states.csv\u0026quot;) %\u0026gt;%\rrename(\u0026quot;StateName\u0026quot; = \u0026quot;State\u0026quot;, \u0026quot;State\u0026quot; = \u0026quot;State Code\u0026quot;) %\u0026gt;% # rename to merge with master dataset\radd_row(\u0026quot;StateName\u0026quot; = \u0026quot;Puerto Rico\u0026quot;, \u0026quot;State\u0026quot; = \u0026quot;PR\u0026quot;, \u0026quot;Region\u0026quot; = \u0026quot;Caribbean \u0026quot;, \u0026quot;Division\u0026quot; = \u0026quot;Caribbean \u0026quot;) #Puerto Rico was not listed in the original Region dataset\rD \u0026lt;- D %\u0026gt;%\rmerge(states, by = \u0026quot;State\u0026quot;, all.x = TRUE)\r\r\rExploratory Data Analysis and Regression Modeling\rThe objective of this exploratory data analysis is to understand the nature of our individual variables. I\nOutcome Variable: Medicare Payments\r\rTotal amount that Medicare paid for hospice care: Hospice services do not have any cost-sharing requirements and the Medicare payment amount will equal the allowed amount.\r\rWhile this dataset provides a metric for total absolute payments, it also provides a standardized payment amont which adjust for geographic differences in payment rates. Here are overall summary statistics for Total Standardized Medicare Payment Amounts.\n\rTotal Medicare Standard Payment Amount: Total amount that Medicare paid for hospice care adjusted for geographic differences in payment rates.\r\rfavstats(D$`Total Medicare Standard Payment Amount`)\r\r\r\rmin\rQ1\rmedian\rQ3\rmax\rmean\rsd\rn\rmissing\r\r\r\r\r12533.5\r841358.2\r2025179\r4398192\r508882642\r4028648\r9584881\r12049\r0\r\r\r\r\rThe mean Standard Payment Amount is 4.028648310^{6}, which is notably higher than the median of 2.025179410^{6}. This fact, along with a large standard deviation of 9.584880810^{6} points towards either a high skew or the presence of some extreme outliers.\nLet’s view Total Medicare Standardized Payment Amount\nplt1 \u0026lt;- D %\u0026gt;%\rggplot(aes(x = Year, y=`Total Medicare Standard Payment Amount`, fill = Year)) + geom_boxplot() + labs(title = \u0026quot;Total Medicare \\nStandared Payment \\nAmount\u0026quot;, fill = \u0026quot;Year\u0026quot;) + ylab(\u0026quot; \u0026quot;) +\rtheme(legend.position=\u0026quot;none\u0026quot;, axis.title = element_text()) + scale_y_continuous(label = dollar) + scale_fill_brewer()\rplt2 \u0026lt;- D %\u0026gt;%\rmutate(rank = rank(desc(`Total Medicare Standard Payment Amount`))) %\u0026gt;%\rarrange(rank) %\u0026gt;%\rfilter(rank \u0026gt; 3) %\u0026gt;%\rggplot(aes(x = Year, y=`Total Medicare Standard Payment Amount`, fill = Year)) + geom_boxplot() + labs(title = \u0026quot;Total Medicare \\nStandared Payment Amount\u0026quot;, subtitle = \u0026quot;Three highest values filtered out\u0026quot;) + ylab(\u0026quot; \u0026quot;) +\rtheme(legend.position=\u0026quot;none\u0026quot;, axis.title = element_text()) +\rscale_y_continuous(label = dollar) + scale_fill_brewer()\r#plot each boxplot series side-by-side using cowplot\rplot_grid(plt1, plt2, align = \u0026quot;h\u0026quot;, rel_widths = c(1, 1.3), axis = \u0026quot;t\u0026quot;)\rIf we remove all outliers, though, Total Medicare Standard Payment Amout mostly resembles a normal distriution.\nplt3 \u0026lt;- D %\u0026gt;%\rmutate(rank = rank(desc(`Total Medicare Standard Payment Amount`))) %\u0026gt;%\rarrange(rank) %\u0026gt;%\rfilter(rank \u0026gt; 3) %\u0026gt;%\rggplot(aes(x = Year, y=`Total Medicare Standard Payment Amount`, fill = Year)) + geom_boxplot(outlier.shape = NA) +\rlabs(title = \u0026quot;Total Medicare \\nStandared Payment Amount\u0026quot;, subtitle = \u0026quot;Three highest values filtered out\u0026quot;) + ylab(\u0026quot; \u0026quot;) +\rtheme(legend.position=\u0026quot;none\u0026quot;, axis.title = element_text()) +\rscale_y_continuous(label = dollar, limits = c(0, 7600000)) +\rscale_fill_brewer()\rplt4 \u0026lt;- D %\u0026gt;%\rmutate(logStandardPaymentAmount = log(`Total Medicare Standard Payment Amount`)) %\u0026gt;%\rggplot(aes(x = Year, y=logStandardPaymentAmount, fill = Year)) + geom_boxplot() + labs(title = \u0026quot;Total Medicare \\nStandared Payment Amount\u0026quot;, subtitle = \u0026quot;Values are log transformed\u0026quot;) + ylab(\u0026quot; \u0026quot;) +\rtheme(legend.position=\u0026quot;none\u0026quot;, axis.title = element_text()) +\rscale_fill_brewer()\rplot_grid(plt3, plt4, align = \u0026quot;h\u0026quot;, rel_widths = c(1, 1.3), axis = \u0026quot;t\u0026quot;)\r## Warning: Removed 1508 rows containing non-finite values (stat_boxplot).\rAfter removing these extreme estreme outliers, Standard Medicare Payment Amount is still skewed.With a normal distribution, log transformed values may be a more useful predictor. It is It is worth noting that the distributions appear to remain farily consistant over the three years in our dataset.\nLet’s see a correlation between our outcome variable and a variables that I would hypothesize would be positively correlated with Total Medicare Standard Payment.\nD \u0026lt;- D %\u0026gt;%\rmutate(logStandardPaymentAmount = log(`Total Medicare Standard Payment Amount`))\rCorVarLabs \u0026lt;- c(\u0026quot;Standard Pmt\u0026quot;, \u0026quot;Hospice beneficiaries\u0026quot;, \u0026quot;Total Days\u0026quot;, \u0026quot;Average Age\u0026quot;, \u0026quot;# with Cancer\u0026quot;)\rD %\u0026gt;%\rselect(`Total Medicare Standard Payment Amount`, `Hospice beneficiaries`, `Total Days`, `Average Age`,\r`Hospice beneficiaries with a primary diagnosis of cancer`) %\u0026gt;% ggpairs(title = \u0026quot;Correlation Matrix with Standard Payment\u0026quot;, columnLabels = CorVarLabs)\rHow do these correlations with the Standard Payment Amount compare with correlations with Logged Standard Payment Amount?\nD %\u0026gt;%\rselect(`logStandardPaymentAmount`, `Hospice beneficiaries`, `Total Days`, `Average Age`,\r`Hospice beneficiaries with a primary diagnosis of cancer`) %\u0026gt;% ggpairs(title = \u0026quot;Correlation Matrix with Logged Standard Payment\u0026quot;, columnLabels = CorVarLabs)\rLogging Standard Payment Amount decreases the correlation these selected predictor variables. It still may be useful in regressoin analyses\n\rPredictor Variables: Data Exploration and Liner Models by Variable Type\rBetween the orignal datasets and merged ones, there are a variety of varialbes to anlayze. This section analyzes each variable of interest, grouped both by variable type (categorical and numeric) and by category (service hours, number of benificiaries by diagnosis, number of benificiaries by race, and number of benificiaries by gender).\nI will analyze each variable individually and compare them to variables within their category using t-tests and/or linear regressions to test differenes within each category.\nCategorical Predictor Variables\rData Exploration\rRegion\nSummarize “Total Medicare Standard Payment Amount” and “Logged Total Medicare Standard Payment Amount” grouped by region and year\nD %\u0026gt;%\rgroup_by(Region) %\u0026gt;%\rsummarise(Min = min(`Total Medicare Standard Payment Amount`),\rQ1 = quantile(`Total Medicare Standard Payment Amount`, .25),\rmedian = median(`Total Medicare Standard Payment Amount`), Q3 = quantile(`Total Medicare Standard Payment Amount`, .75),\rmax = max(`Total Medicare Standard Payment Amount`),\rmean = mean(`Total Medicare Standard Payment Amount`),\rsd = sd(`Total Medicare Standard Payment Amount`),\rn = n(),\rmissing = count(is.na(`Total Medicare Standard Payment Amount`))) %\u0026gt;%\rarrange(desc(mean))\r## Warning: `data_frame()` is deprecated as of tibble 1.1.0.\r## Please use `tibble()` instead.\r## This warning is displayed once every 8 hours.\r## Call `lifecycle::last_warnings()` to see where this warning was generated.\r## `summarise()` ungrouping output (override with `.groups` argument)\r\r\rRegion\rMin\rQ1\rmedian\rQ3\rmax\rmean\rsd\rn\rmissing\r\r\r\rNortheast\r66370.61\r1130719.3\r2469996\r5488644\r55069310\r4701323\r6381407\r1332\r0\r\rSouth\r12533.50\r997042.3\r2214590\r4494683\r508882642\r4531637\r13678330\r4750\r0\r\rCaribbean\r139677.62\r1643344.3\r2743352\r3791254\r42399624\r4468607\r7002663\r115\r0\r\rMidwest\r21943.54\r752023.8\r1873480\r4427457\r72307285\r3828595\r5702301\r2764\r0\r\rWest\r20066.94\r609593.0\r1506441\r3709664\r43524824\r3127468\r4562045\r3088\r0\r\r\r\r\rThe Northeast has the highest mean payment amounts and second highest median, behind the Caribbean. The South has the widest spread. This seems natural since it has the highest amount of providers as well. The West has the lowest amount mean.\nLinear Model Exploring Payment by Region\rlm(`Total Medicare Standard Payment Amount` ~ Region, data = D) %\u0026gt;%\rget_regression_table()\r\r\rterm\restimate\rstd_error\rstatistic\rp_value\rlower_ci\rupper_ci\r\r\r\rintercept\r4468607.33\r892146.3\r5.009\r0.000\r2719857\r6217357.6\r\rRegionMidwest\r-640012.78\r910516.6\r-0.703\r0.482\r-2424772\r1144746.4\r\rRegionNortheast\r232715.77\r929861.4\r0.250\r0.802\r-1589962\r2055393.8\r\rRegionSouth\r63029.46\r902881.4\r0.070\r0.944\r-1706763\r1832822.2\r\rRegionWest\r-1341139.41\r908606.6\r-1.476\r0.140\r-3122155\r439875.8\r\r\r\r\rYear\nD %\u0026gt;%\rgroup_by(Year) %\u0026gt;%\rsummarise(Min = min(`Total Medicare Standard Payment Amount`),\rQ1 = quantile(`Total Medicare Standard Payment Amount`, .25),\rmedian = median(`Total Medicare Standard Payment Amount`), Q3 = quantile(`Total Medicare Standard Payment Amount`, .75),\rmax = max(`Total Medicare Standard Payment Amount`),\rmean = mean(`Total Medicare Standard Payment Amount`),\rsd = sd(`Total Medicare Standard Payment Amount`),\rn = n(),\rmissing = count(is.na(`Total Medicare Standard Payment Amount`))) %\u0026gt;%\rarrange(desc(mean))\r## `summarise()` ungrouping output (override with `.groups` argument)\r\r\rYear\rMin\rQ1\rmedian\rQ3\rmax\rmean\rsd\rn\rmissing\r\r\r\r2016\r21943.54\r864861.5\r2081425\r4575784\r508882642\r4159611\r10133316\r4125\r0\r\r2015\r12533.50\r850154.8\r1998732\r4354607\r473266501\r4028250\r9650059\r4009\r0\r\r2014\r12541.04\r812344.8\r1977470\r4294485\r416553977\r3891069\r8899534\r3915\r0\r\r\r\r\rThe mean and median payment accross the board has increased incrementally each year.\nIs this difference significant in a linear model?\nlm(`Total Medicare Standard Payment Amount` ~ Year, data = D) %\u0026gt;%\rget_regression_table()\r\r\rterm\restimate\rstd_error\rstatistic\rp_value\rlower_ci\rupper_ci\r\r\r\rintercept\r4159610.9\r149239.0\r27.872\r0.000\r3867078.4\r4452143.4\r\rYear2015\r-131361.0\r212577.1\r-0.618\r0.537\r-548046.3\r285324.2\r\rYear2014\r-268541.9\r213867.4\r-1.256\r0.209\r-687756.4\r150672.5\r\r\r\r\rWith insignificant p-vlaues for each of the years, there is not a significant difference by year.\n\r\rLet’s break down Average Payment by Region and Year\rD %\u0026gt;%\rgroup_by(Region, Year) %\u0026gt;%\rsummarise(Payment = mean(`Total Medicare Standard Payment Amount`)) %\u0026gt;%\rggplot(aes(x = as.numeric(as.character(Year)), y = Payment, color= Region)) +\rgeom_line(size = 0.9) + geom_point() + labs(title = \u0026quot;Mean Total Medicare Standard Payment Amount by Region\u0026quot;) + xlab(\u0026quot;Year\u0026quot;) + ylab(\u0026quot;Mean Payment\u0026quot;) +\rscale_x_continuous(labels = c(\u0026quot;2014.0\u0026quot; = \u0026quot;2014\u0026quot;, \u0026quot;2014.5\u0026quot; = \u0026quot; \u0026quot;, \u0026quot;2015.0\u0026quot; = \u0026quot;2015\u0026quot;, \u0026quot;2015.5\u0026quot; = \u0026quot; \u0026quot;, \u0026quot;2016.0\u0026quot; = \u0026quot;2016\u0026quot;)) +\rscale_y_continuous(label = dollar) +\rscale_color_brewer()\r## `summarise()` regrouping output by \u0026#39;Region\u0026#39; (override with `.groups` argument)\rPayments are growing each year accross each region, except the West with a slight decline.\nIntertion Rregression with Year and Region\rLet’s see if that gorwing progression holds up in an interaction linear model\nlm(`Total Medicare Standard Payment Amount` ~ Year*Region, data = D) %\u0026gt;%\rget_regression_table()\r\r\rterm\restimate\rstd_error\rstatistic\rp_value\rlower_ci\rupper_ci\r\r\r\rintercept\r4409769.18\r1532382\r2.878\r0.004\r1406053\r7413485\r\rYear2015\r135412.27\r2167116\r0.062\r0.950\r-4112484\r4383308\r\rYear2014\r40143.49\r2196206\r0.018\r0.985\r-4264774\r4345061\r\rRegionMidwest\r-365963.59\r1564422\r-0.234\r0.815\r-3432482\r2700555\r\rRegionNortheast\r637839.66\r1599300\r0.399\r0.690\r-2497046\r3772725\r\rRegionSouth\r333632.32\r1551015\r0.215\r0.830\r-2706607\r3373872\r\rRegionWest\r-1329208.91\r1558556\r-0.853\r0.394\r-4384229\r1725812\r\rYear2015:RegionMidwest\r-380649.59\r2212426\r-0.172\r0.863\r-4717361\r3956062\r\rYear2014:RegionMidwest\r-441547.91\r2241049\r-0.197\r0.844\r-4834365\r3951269\r\rYear2015:RegionNortheast\r-524332.08\r2261021\r-0.232\r0.817\r-4956297\r3907633\r\rYear2014:RegionNortheast\r-679993.19\r2288218\r-0.297\r0.766\r-5165270\r3805284\r\rYear2015:RegionSouth\r-324338.27\r2193558\r-0.148\r0.882\r-4624065\r3975388\r\rYear2014:RegionSouth\r-489483.91\r2222384\r-0.220\r0.826\r-4845715\r3866748\r\rYear2015:RegionWest\r-90756.49\r2206145\r-0.041\r0.967\r-4415156\r4233643\r\rYear2014:RegionWest\r65948.32\r2236529\r0.029\r0.976\r-4318009\r4449906\r\r\r\r\rNeither levels themselves nor their interactions have statistically significant p-values, so alone these are not great predictors. This is actually something of positive sign for the industry. It would be concerning if amount paid varied drastically by year or region given the nature of the service hospice centers provide, so these are slightly encouraging findings.\n\r\r\r\rNumeric Predictors of Interest\rPhysician Services\r\rPhysician Services: “Total number of hospice care physician services provided”\r\rfav_stats(D$`Physician Services`)\r\r\r\rmin\rQ1\rmedian\rQ3\rmax\rmean\rsd\rn\rmissing\r\r\r\r\r0\r0\r0\r54\r100641\r306.6639\r1952.223\r10803\r1246\r\r\r\r\rThere appears to be a very wide range with a large standard deviation. This variable is very skewed since both Q1 and the median are 0 hours, but the mean is about 307 hours.\nD %\u0026gt;%\rggplot(aes(y=`Physician Services`)) + geom_boxplot() +\rlabs(title = \u0026quot;Physician Services\u0026quot;,\rsubtitle = \u0026quot;Total number of hospice care physician services provided\u0026quot;) +\rylab(\u0026quot;Physician Service Hours\u0026quot;)\r## Warning: Removed 1246 rows containing non-finite values (stat_boxplot).\rIndeed, there are several outliers present.\n\rCare Hours Spent\r\rHome Health Visit Hours per Day\r\rfav_stats(D$`Home Health Visit Hours per Day`)\r\r\r\rmin\rQ1\rmedian\rQ3\rmax\rmean\rsd\rn\rmissing\r\r\r\r\r0\r0.19\r0.27\r0.37\r3.48\r0.3133646\r0.2269501\r12049\r0\r\r\r\r\r\rHome Health Visit Hours per Day During Week Prior to Death\r\rfav_stats(D$`Home Health Visit Hours per Day During Week Prior to Death`)\r\r\r\rmin\rQ1\rmedian\rQ3\rmax\rmean\rsd\rn\rmissing\r\r\r\r\r0\r0.19\r0.28\r0.42\r8\r0.3625512\r0.3419532\r12049\r0\r\r\r\r\rIt is known that overall visit hours and, thus payments, are higher in th last week of life. Is that supported in this data?\nt.test(D$`Home Health Visit Hours per Day`, D$`Home Health Visit Hours per Day During Week Prior to Death`) %\u0026gt;% list() %\u0026gt;% map_df(.,tidy)\r\r\restimate\restimate1\restimate2\rstatistic\rp.value\rparameter\rconf.low\rconf.high\rmethod\ralternative\r\r\r\r-0.0491867\r0.3133646\r0.3625512\r-13.15535\r0\r20937.13\r-0.0565152\r-0.0418581\rWelch Two Sample t-test\rtwo.sided\r\r\r\r\rYes, the expected amount of visit hours is significanlty higher in the last week prior to death than it is overal. There is an average increase in Home Health Visit Hours per day of 0.05 hours in the week prior to death. Given the circumstances, that is actually a bit less of an increase than I would have expected.\rLets see if there is a greter increase in other types of care hours in the week prior to death.\nD %\u0026gt;%\rselect(`Provider ID`,\r`Home Health Visit Hours per Day`, `Home Health Visit Hours per Day During Week Prior to Death`) %\u0026gt;%\rgather(key = \u0026quot;Type\u0026quot;, value = \u0026quot;Values\u0026quot;, 2:3) %\u0026gt;%\rggplot(aes(x=Values, fill = Type)) + geom_density() +\rggtitle(\u0026quot;Home Health Visit Hours per Day\u0026quot;) +\rtheme(legend.position=\u0026quot;bottom\u0026quot;)\rThe density plot of their logged values show that there is only a slight increase in hours per week in the week prior to death. This is likely due to a few outliers as well.\n\rSkilled Nursing Visit Hours per Day\r\rfav_stats(D$`Skilled Nursing Visit Hours per Day`)\r\r\r\rmin\rQ1\rmedian\rQ3\rmax\rmean\rsd\rn\rmissing\r\r\r\r\r0\r0.21\r0.26\r0.33\r5.39\r0.2856171\r0.1708162\r12049\r0\r\r\r\r\r\rSkilled Nursing Visit Hours per Day During Week Prior to Death\r\rfav_stats(D$`Skilled Nursing Visit Hours per Day During Week Prior to Death`)\r\r\r\rmin\rQ1\rmedian\rQ3\rmax\rmean\rsd\rn\rmissing\r\r\r\r\r0\r0.6\r0.8\r1.06\r19.37\r1.035322\r1.042784\r12049\r0\r\r\r\r\rDoes the hypothesized difference stand for Skilled Nursing Visits?\nt.test(D$`Skilled Nursing Visit Hours per Day`, D$`Skilled Nursing Visit Hours per Day During Week Prior to Death`) %\u0026gt;% list() %\u0026gt;% map_df(.,tidy) \r\r\restimate\restimate1\restimate2\rstatistic\rp.value\rparameter\rconf.low\rconf.high\rmethod\ralternative\r\r\r\r-0.7497054\r0.2856171\r1.035322\r-77.87929\r0\r12694.1\r-0.7685748\r-0.730836\rWelch Two Sample t-test\rtwo.sided\r\r\r\r\rYes, there is a statistically significant increase in average hours per day by skilled nurses in the week prior to death than there are overall by 0.75 hours. This continues our expected trend of a slight increase in care hours in the week prior to death.\n\rSocial Service Visit Hours per Day\r\rfav_stats(D$`Social Service Visit Hours per Day`)\r\r\r\rmin\rQ1\rmedian\rQ3\rmax\rmean\rsd\rn\rmissing\r\r\r\r\r0\r0.03\r0.04\r0.06\r0.47\r0.0432675\r0.0255404\r12049\r0\r\r\r\r\r\rSocial Service Visit Hours per Day During Week Prior to Death\r\rfav_stats(D$`Social Service Visit Hours per Day During Week Prior to Death`)\r\r\r\rmin\rQ1\rmedian\rQ3\rmax\rmean\rsd\rn\rmissing\r\r\r\r\r0\r0.05\r0.08\r0.12\r1.07\r0.0933679\r0.0665678\r12049\r0\r\r\r\r\rDoes the difference hold for Social Service Hours per Day?\nt.test(D$`Social Service Visit Hours per Day`, D$`Social Service Visit Hours per Day During Week Prior to Death`) %\u0026gt;% list() %\u0026gt;% map_df(.,tidy)\r\r\restimate\restimate1\restimate2\rstatistic\rp.value\rparameter\rconf.low\rconf.high\rmethod\ralternative\r\r\r\r-0.0501004\r0.0432675\r0.0933679\r-77.13154\r0\r15519.86\r-0.0513736\r-0.0488272\rWelch Two Sample t-test\rtwo.sided\r\r\r\r\rYes. Continuing this trend, there is an average incraese in hours visited by social service representatives in the week prior to death.\nLinear Regression by type of hours spent with patients\rmVisits \u0026lt;- lm(`Total Medicare Standard Payment Amount` ~ `Home Health Visit Hours per Day` +\r`Home Health Visit Hours per Day During Week Prior to Death` +\r`Skilled Nursing Visit Hours per Day` +\r`Skilled Nursing Visit Hours per Day During Week Prior to Death` +\r`Social Service Visit Hours per Day` +\r`Social Service Visit Hours per Day During Week Prior to Death`, data = D)\rget_regression_table(mVisits)\r\r\rterm\restimate\rstd_error\rstatistic\rp_value\rlower_ci\rupper_ci\r\r\r\rintercept\r547225.3\r215837.7\r2.535\r0.011\r124148.7\r970301.9\r\rHome Health Visit Hours per Day\r-2526305.4\r526784.5\r-4.796\r0.000\r-3558887.8\r-1493723.1\r\rHome Health Visit Hours per Day During Week Prior to Death\r5282354.9\r354670.5\r14.894\r0.000\r4587143.6\r5977566.3\r\rSkilled Nursing Visit Hours per Day\r832342.8\r639300.5\r1.302\r0.193\r-420789.1\r2085474.7\r\rSkilled Nursing Visit Hours per Day During Week Prior to Death\r1349556.5\r99011.4\r13.630\r0.000\r1155478.2\r1543634.8\r\rSocial Service Visit Hours per Day\r-9206518.6\r5057542.3\r-1.820\r0.069\r-19120115.8\r707078.6\r\rSocial Service Visit Hours per Day During Week Prior to Death\r12009895.1\r1890417.2\r6.353\r0.000\r8304373.0\r15715417.1\r\r\r\r\rThis linear model is quite interesting and suprising to me. Each of the types of hours are statistically significant except Skilled Nursing Visit Hours per day, and Social Service Visit Hous per Day. Suprisingly, the amount of Home Health Visit Hours per Day and Social Service Visit Hours per Day had significant negaitve coefficients. For Home Health Visit Hours per Day, for each increase hour visited, we expect a decrease in standard payment amount by 2526305.4. However, this linear model alone has a low explanatory capacity, explaining only 7% of the overall variation in Total Medicare Standard Payment Amount. To assess if this is the result of very skewed data among all predictor and outcome variales, I will rerun the model using logged predictor and outcome values.\n\rRegression with logged values\r\r# create new dataframe for regression analysis of logged values\rDbyVistLog \u0026lt;- D %\u0026gt;%\r#### select only needed variables\rselect(`Provider ID`,\r`Total Medicare Standard Payment Amount`,\r`Home Health Visit Hours per Day`,\r`Home Health Visit Hours per Day During Week Prior to Death`,\r`Skilled Nursing Visit Hours per Day`, `Skilled Nursing Visit Hours per Day During Week Prior to Death`, `Social Service Visit Hours per Day`, `Social Service Visit Hours per Day During Week Prior to Death`) %\u0026gt;%\r### filter out NAs\rfilter(!is.na(`Total Medicare Standard Payment Amount`) \u0026amp; !is.na(`Home Health Visit Hours per Day`) \u0026amp; !is.na(`Home Health Visit Hours per Day During Week Prior to Death`) \u0026amp;\r!is.na(`Skilled Nursing Visit Hours per Day`) \u0026amp;\r!is.na(`Skilled Nursing Visit Hours per Day During Week Prior to Death`) \u0026amp; !is.na(`Social Service Visit Hours per Day`) \u0026amp;\r!is.na(`Social Service Visit Hours per Day During Week Prior to Death`)) %\u0026gt;%\r### mutate with logging each value plus 1 (since min of multiple variables is 0)\rmutate(lpmt = (log((`Total Medicare Standard Payment Amount`)+1)),\rlHHVisitHRs = (log((`Home Health Visit Hours per Day`)+1)),\rlHHVisitHRsD = (log((`Home Health Visit Hours per Day During Week Prior to Death`)+1)),\rlSNV = (log((`Skilled Nursing Visit Hours per Day`)+1)),\rlSNVD = (log((`Skilled Nursing Visit Hours per Day During Week Prior to Death`)+1)),\rlSSVH = (log((`Social Service Visit Hours per Day`)+1)),\rlSSVHD = (log((`Social Service Visit Hours per Day During Week Prior to Death`)+1)))\r# create and output model\rmVisitsL \u0026lt;- lm(lpmt ~ lHHVisitHRs + lHHVisitHRsD + lHHVisitHRsD + lSNV + lSNVD + lSSVH + lSSVHD, data = DbyVistLog)\rget_regression_table(mVisitsL)\r\r\rterm\restimate\rstd_error\rstatistic\rp_value\rlower_ci\rupper_ci\r\r\r\rintercept\r13.789\r0.034\r410.747\r0.000\r13.723\r13.855\r\rlHHVisitHRs\r0.222\r0.120\r1.851\r0.064\r-0.013\r0.457\r\rlHHVisitHRsD\r1.279\r0.095\r13.447\r0.000\r1.093\r1.465\r\rlSNV\r-0.705\r0.149\r-4.716\r0.000\r-0.997\r-0.412\r\rlSNVD\r0.413\r0.047\r8.867\r0.000\r0.321\r0.504\r\rlSSVH\r-6.403\r0.723\r-8.859\r0.000\r-7.820\r-4.986\r\rlSSVHD\r4.767\r0.291\r16.394\r0.000\r4.197\r5.337\r\r\r\r\rEven after logging the values, a Social Service Visit Hours per Day still displays negative correlation with Total Medicare Standard Payment Amount. Interpreting the logged regression, each percent increase in Social Service Visit hours predicts a 6.403% decrease in Total Medicare Standard Payment Amount. After logging variables, this model also shows a predicted slight percent decrease in Total Medicare Standard Payment Amount with each percent increase in Skilled Nursing Visit Hours.\nModel Diagnostics:\nget_regression_summaries(mVisitsL)\r\r\rr_squared\radj_r_squared\rmse\rrmse\rsigma\rstatistic\rp_value\rdf\r\r\r\r0.097\r0.097\r1.414159\r1.189184\r1.19\r216.597\r0\r7\r\r\r\r\rget_regression_points(mVisitsL) %\u0026gt;%\rggplot(aes(x=residual)) + geom_density() +\rggtitle(\u0026quot;Residuals from mVisits Regression Model\u0026quot;) \rEven the logged model still has a low overall predicitve capacity with an R Squared of only 10%. The model as a whole is statistically significant and the residuals do appear to be normal.\n\r\rDiagnosis Types\r# Create tidy dataframe with number of beneficiaries by type for easier analysis\rDbyDiagnosis \u0026lt;- D %\u0026gt;%\rselect(`Provider ID`,\r`Hospice beneficiaries with a primary diagnosis of cancer`, `Hospice beneficiaries with a primary diagnosis of dementia`,\r`Hospice beneficiaries with a primary diagnosis of stroke`,\r`Hospice beneficiaries with a primary diagnosis of circulatory/heart disease`,\r`Hospice beneficiaries with a primary diagnosis of respiratory disease`) %\u0026gt;%\rgather(key = \u0026quot;Type\u0026quot;, value = \u0026quot;Beneficiaries\u0026quot;, 2:6)\rSummary Statistics by Diagnosis Type\nDbyDiagnosis %\u0026gt;% group_by(Type) %\u0026gt;%\rfilter(!is.na(Beneficiaries)) %\u0026gt;%\rsummarise(Min = min(Beneficiaries),\rQ1 = quantile(Beneficiaries, .25),\rmedian = median(Beneficiaries), Q3 = quantile(Beneficiaries, .75),\rmax = max(Beneficiaries),\rmean = mean(Beneficiaries),\rsd = sd(Beneficiaries),\rn = n()) %\u0026gt;%\rarrange(desc(mean))\r## `summarise()` ungrouping output (override with `.groups` argument)\r\r\rType\rMin\rQ1\rmedian\rQ3\rmax\rmean\rsd\rn\r\r\r\rHospice beneficiaries with a primary diagnosis of cancer\r0\r27\r54\r125\r5426\r117.23597\r205.04688\r10048\r\rHospice beneficiaries with a primary diagnosis of dementia\r0\r24\r46\r94\r2078\r83.31137\r120.07482\r9362\r\rHospice beneficiaries with a primary diagnosis of circulatory/heart disease\r0\r24\r44\r88\r4993\r78.73133\r130.76801\r9867\r\rHospice beneficiaries with a primary diagnosis of respiratory disease\r0\r19\r33\r65\r2398\r57.41664\r83.78012\r7546\r\rHospice beneficiaries with a primary diagnosis of stroke\r0\r15\r27\r54\r8336\r52.11899\r158.85912\r6757\r\r\r\r\rDbyDiagnosis %\u0026gt;%\rggplot(aes(x = Type, y = Beneficiaries, color = Type)) + geom_boxplot() + theme(legend.position=\u0026quot;none\u0026quot;) +\rscale_x_discrete(labels = c(\u0026quot;Cancer\u0026quot;, \u0026quot;Dementia\u0026quot;, \u0026quot;Stroke\u0026quot;, \u0026quot;Circulatory/Heart\u0026quot;, \u0026quot;Respiratory\u0026quot;)) + labs(title = \u0026quot;Number of Benificiaries by Primary Diagnosis\u0026quot;) +\rylab(\u0026quot;Number of Distinct Beneficiaries\u0026quot;) +\rscale_color_brewer()\rThere are some extreme outliers, especially in Cancer, Dementia, and Respiratory\nDbyDiagnosis %\u0026gt;%\rggplot(aes(x = Type, y = log(Beneficiaries), fill = Type)) + geom_boxplot(outlier.shape = NA) + scale_x_discrete(labels = c(\u0026quot;Cancer\u0026quot;, \u0026quot;Dementia\u0026quot;, \u0026quot;Stroke\u0026quot;, \u0026quot;Circulatory/Heart\u0026quot;, \u0026quot;Respiratory\u0026quot;)) + labs(title = \u0026quot;Number of Benificiaries by Primary Diagnosis\u0026quot;,\rsubtitle = \u0026quot;Values are Logged\u0026quot;) +\rylab(\u0026quot;Number of Distinct Beneficiaries\u0026quot;) + theme(legend.position = \u0026quot;none\u0026quot;) +\rscale_color_brewer()\rAfter logging, the distribution acorss primary diagnosis type is both consistant and normal between diagnosies.\n\rRegression by Diagnosis\rDiagMod \u0026lt;- lm(log((`Total Medicare Standard Payment Amount`)+1) ~ log((`Hospice beneficiaries with a primary diagnosis of cancer`)+1) + log((`Hospice beneficiaries with a primary diagnosis of dementia`)+1) +\rlog((`Hospice beneficiaries with a primary diagnosis of stroke`)+1) +\rlog((`Hospice beneficiaries with a primary diagnosis of circulatory/heart disease`)+1) +\rlog((`Hospice beneficiaries with a primary diagnosis of respiratory disease`)+1), data = D)\rget_regression_table(DiagMod) \r\r\rterm\restimate\rstd_error\rstatistic\rp_value\rlower_ci\rupper_ci\r\r\r\rintercept\r11.231\r0.019\r591.584\r0\r11.194\r11.268\r\rlog((Hospice beneficiaries with a primary diagnosis of cancer) + 1)\r-0.072\r0.007\r-10.481\r0\r-0.086\r-0.059\r\rlog((Hospice beneficiaries with a primary diagnosis of dementia) + 1)\r0.362\r0.006\r63.777\r0\r0.351\r0.373\r\rlog((Hospice beneficiaries with a primary diagnosis of stroke) + 1)\r0.195\r0.007\r29.524\r0\r0.182\r0.208\r\rlog((Hospice beneficiaries with a primary diagnosis of circulatory/heart disease) + 1)\r0.362\r0.009\r38.904\r0\r0.344\r0.381\r\rlog((Hospice beneficiaries with a primary diagnosis of respiratory disease) + 1)\r0.162\r0.009\r18.099\r0\r0.145\r0.180\r\r\r\r\rThis logged model shows several diagnosies with minimal predictive power. Each coefficient is statistically significant at a 0.01 alpha level. Beneficiaries with primary diagnoses of cancer or circulatory/heart had the highest impact on Total Medicare Standard Payment Amount; with each percent increase in the number of benificiaries with either cancer or circulatory/heart disease, we would expect a 0.362 percent increase in Total Medicare Standard Payment Amount. While this may seem small, this impact adds up over time!\n\rGender Distribution\rDbyGender \u0026lt;- D %\u0026gt;%\rselect(`Provider ID`, `Female hospice beneficiaries`, `Male hospice beneficiaries`) %\u0026gt;%\rgather(key = \u0026quot;Gender\u0026quot;, value = \u0026quot;Amount\u0026quot;, 2:3) %\u0026gt;%\rfilter(!is.na(Amount)) \rDbyGender %\u0026gt;% group_by(Gender) %\u0026gt;%\rsummarise(Min = min(Amount),\rQ1 = quantile(Amount, .25),\rmedian = median(Amount), Q3 = quantile(Amount, .75),\rmax = max(Amount),\rmean = mean(Amount),\rsd = sd(Amount),\rn = n(),\rmissing = count(is.na(Amount))) %\u0026gt;%\rarrange(desc(mean))\r## `summarise()` ungrouping output (override with `.groups` argument)\r\r\rGender\rMin\rQ1\rmedian\rQ3\rmax\rmean\rsd\rn\rmissing\r\r\r\rFemale hospice beneficiaries\r11\r54\r114\r246\r15554\r224.6755\r407.2776\r11116\r0\r\rMale hospice beneficiaries\r11\r37\r79\r172\r10325\r157.8419\r287.6987\r11116\r0\r\r\r\r\rDbyGender %\u0026gt;%\rggplot(aes(x=Gender, y = Amount, color = Gender)) +\rgeom_boxplot() + labs(title = \u0026quot;Number of Hospice Benificiaries by Gender\u0026quot;) +\rtheme(legend.position=\u0026quot;none\u0026quot;) +\rscale_color_brewer()\r# count outliers by gender\rlibrary(grDevices)\rF \u0026lt;- DbyGender %\u0026gt;% filter(Gender == \u0026quot;Female hospice beneficiaries\u0026quot;)\rFL \u0026lt;- length(F$Amount[which(F$Amount %in% boxplot.stats(F$Amount)$out)])\rM \u0026lt;- DbyGender %\u0026gt;% filter(Gender == \u0026quot;Male hospice beneficiaries\u0026quot;)\rML \u0026lt;- length(M$Amount[which(M$Amount %in% boxplot.stats(M$Amount)$out)])\rThe distribution appears to be skewed, but we have 1004 outliers for the amount of female benificiaries and 1040 outliers for the amount of male benificiaries.\nDbyGender %\u0026gt;%\rggplot(aes(x=Gender, y = Amount, fill = Gender)) +\rgeom_boxplot(outlier.shape = NA) + ylim(c(0,400)) +\rlabs(title = \u0026quot;Number of Hospice Benificiaries by Gender\u0026quot;, subtitle = \u0026quot;Outliers Removed\u0026quot;) +\rtheme(legend.position=\u0026quot;none\u0026quot;) +\rscale_fill_brewer()\r## Warning: Removed 2471 rows containing non-finite values (stat_boxplot).\rDistribution is still somewhat skewed, but resembles closer to a normal distribution.\nRegression model by Gender\rIs there a difference in impact on Total Standard Payment by number of benificiaries by gender.\nGMod \u0026lt;- lm(log(`Total Medicare Standard Payment Amount`) ~ log(`Female hospice beneficiaries`) +\rlog(`Male hospice beneficiaries`), data = D)\rget_regression_table(GMod) \r\r\rterm\restimate\rstd_error\rstatistic\rp_value\rlower_ci\rupper_ci\r\r\r\rintercept\r10.046\r0.015\r652.148\r0.000\r10.016\r10.076\r\rlog(Female hospice beneficiaries)\r0.922\r0.013\r72.061\r0.000\r0.897\r0.947\r\rlog(Male hospice beneficiaries)\r0.042\r0.013\r3.278\r0.001\r0.017\r0.067\r\r\r\r\rIn using log values, we see a 0.88% stronger impact of female beneficiaries on Total Medicare Standard Payment Amount than we do for male beneficiaries. Both factors are statistically significant, though. This model alone, though, has a very strong exlanatory capacity with an R squared at 90%. That is by far the strongest model within each category.\nAll told, this indicates that there are noteably more women than men in the US hospice system.\n\r\rRacial Distributions\rDbyRace \u0026lt;- D %\u0026gt;%\rselect(`Provider ID`, `White hospice beneficiaries`, `Black hospice beneficiaries`, `Asian hospice beneficiaries`, `Hispanic hospice beneficiaries`, `Other/unknown race hospice beneficiaries`) %\u0026gt;%\rgather(key = \u0026quot;Race\u0026quot;, value = \u0026quot;Amount\u0026quot;, 2:6) %\u0026gt;%\rfilter(!is.na(Amount))\rDbyRace %\u0026gt;%\rgroup_by(Race) %\u0026gt;%\rsummarise(Min = min(Amount),\rQ1 = quantile(Amount, .25),\rmedian = median(Amount), Q3 = quantile(Amount, .75),\rmax = max(Amount),\rmean = mean(Amount),\rsd = sd(Amount),\rn = n(),\rmissing = count(is.na(Amount))) %\u0026gt;%\rarrange(desc(mean))\r## `summarise()` ungrouping output (override with `.groups` argument)\r\r\rRace\rMin\rQ1\rmedian\rQ3\rmax\rmean\rsd\rn\rmissing\r\r\r\rWhite hospice beneficiaries\r0\r63\r147\r335\r16332\r306.110458\r539.10949\r11570\r0\r\rBlack hospice beneficiaries\r0\r0\r19\r48\r2917\r45.129267\r98.52297\r7295\r0\r\rHispanic hospice beneficiaries\r0\r0\r13\r37\r6145\r43.443305\r171.20910\r5556\r0\r\rAsian hospice beneficiaries\r0\r0\r0\r0\r1230\r7.722306\r40.40040\r5967\r0\r\rOther/unknown race hospice beneficiaries\r0\r0\r0\r11\r234\r6.733256\r15.77411\r3449\r0\r\r\r\r\rDbyRace$RaceLabel \u0026lt;- word(DbyRace$Race, 1) #extract the first word in the varaible, caputring the only word needed for the boxplot label\rDbyRace %\u0026gt;% ggplot(aes(x=RaceLabel, y = Amount, color = Race)) +\rgeom_boxplot() + labs(title = \u0026quot;Number of Hospice Benificiaries by Race\u0026quot;) +\rtheme(legend.position=\u0026quot;none\u0026quot;) + xlab(\u0026quot;Race\u0026quot;) + ylab(\u0026quot;Amount of Beifiareis\u0026quot;) +\rscale_color_brewer()\rDbyRace %\u0026gt;% ggplot(aes(x=RaceLabel, y = Amount, fill = Race)) +\rgeom_boxplot(outlier.shape = NA) + labs(title = \u0026quot;Number of Hospice Benificiaries by Race\u0026quot;,\rsubtitle = \u0026quot;Outliers Removed\u0026quot;) +\rtheme(legend.position=\u0026quot;none\u0026quot;) + ylim(c(0, 550)) +\rxlab(\u0026quot;Race\u0026quot;) + ylab(\u0026quot;Amount of Beifiareis\u0026quot;) +\rscale_fill_brewer()\r## Warning: Removed 1766 rows containing non-finite values (stat_boxplot).\rIt is difficult to see the distribution of amount of benificiaries by race, particularly considering the range for amount of White identifying benificiaries is so much higher than it is for the each of the other Race categories.\nDbyRace %\u0026gt;%\rgroup_by(Race) %\u0026gt;% summarise(total = sum(Amount)) %\u0026gt;%\rmutate(percent = percent(total/sum(total)))\r## `summarise()` ungrouping output (override with `.groups` argument)\r\r\rRace\rtotal\rpercent\r\r\r\rAsian hospice beneficiaries\r46079\r1.10%\r\rBlack hospice beneficiaries\r329218\r7.87%\r\rHispanic hospice beneficiaries\r241371\r5.77%\r\rOther/unknown race hospice beneficiaries\r23223\r0.56%\r\rWhite hospice beneficiaries\r3541698\r84.70%\r\r\r\r\rFor a clearer visualization of this descrepency, let’s see the difference just between White and White.\nDbyRace \u0026lt;- DbyRace %\u0026gt;%\rmutate(White = RaceLabel == \u0026quot;White\u0026quot;)\rDbyRace %\u0026gt;%\rgroup_by(White) %\u0026gt;% summarise(total = sum(Amount)) %\u0026gt;%\rmutate(percent = percent(total/sum(total)))\r## `summarise()` ungrouping output (override with `.groups` argument)\r\r\rWhite\rtotal\rpercent\r\r\r\rFALSE\r639891\r15%\r\rTRUE\r3541698\r85%\r\r\r\r\rDbyRace %\u0026gt;% ggplot(aes(x=White, y = Amount, fill = White)) +\rgeom_boxplot(outlier.shape = NA) + labs(title = \u0026quot;Number of Hospice Benificiaries by Race\u0026quot;,\rsubtitle = \u0026quot;Outliers Removed, Non-White and White\u0026quot;) +\rtheme(legend.position=\u0026quot;none\u0026quot;) + ylim(c(0, 550)) +\rxlab(\u0026quot;White\u0026quot;) + ylab(\u0026quot;Amount of Beifiareis\u0026quot;) +\rscale_fill_brewer()\r## Warning: Removed 1766 rows containing non-finite values (stat_boxplot).\rIt is interesting to note that 84.7% of the beneficiaries in the dataset are identified as white. This could be an anomolie or the result of several intersecting socioeconoimc and historical factors pertaining to access to healh care, especially end-of-life care. This is consistant Cohen’s 2008 systematic review of racial disparities in hospice care system.\nLet’s see how this logic holds up in a regression model on Total Medicare Standard Payment Amount.\nRMod \u0026lt;- lm(`Total Medicare Standard Payment Amount` ~ `White hospice beneficiaries` +\r`Black hospice beneficiaries` + `Asian hospice beneficiaries` +\r`Hispanic hospice beneficiaries` + `Other/unknown race hospice beneficiaries`, data = D)\rget_regression_table(RMod)\r\r\rterm\restimate\rstd_error\rstatistic\rp_value\rlower_ci\rupper_ci\r\r\r\rintercept\r-999832.977\r136345.786\r-7.333\r0.000\r-1267300.053\r-732365.90\r\rWhite hospice beneficiaries\r10677.084\r202.108\r52.829\r0.000\r10280.612\r11073.56\r\rBlack hospice beneficiaries\r11347.950\r1113.098\r10.195\r0.000\r9164.406\r13531.50\r\rAsian hospice beneficiaries\r5467.372\r3376.764\r1.619\r0.106\r-1156.766\r12091.51\r\rHispanic hospice beneficiaries\r44770.962\r618.849\r72.345\r0.000\r43556.976\r45984.95\r\rOther/unknown race hospice beneficiaries\r-76587.338\r17497.609\r-4.377\r0.000\r-110912.082\r-42262.59\r\r\r\r\rsummary(RMod)$r.squared\r## [1] 0.9701032\rThis model has a vey high predictive power with an R squared of 97% with each coefficient being statistically significant. According to this model, each added white beneficiary contributes an expected $10677.08 in standard medicare payments with a relatively low standard error of $202.11. Each added added Black beneficiary contributes an expected $11347.95 in standard medicare payments, but with a relatively high standard error of $1113.10. This variation is again likely due to the notably more white beneficiares in the dataset.\n\r\rFinal Conclusions and Takeaways\r\rDiagnosis Type, Gender, and Race all proved to be strong predictors of standard medicare payments\rYear and Region did not prove to be strong predictors.\rThere are noteably more women then men recieving treatment in the hospice system.\rHospice payments are tend to be skewed with a small group of highlgy differentiated outliers. If one is interested in detecting fraud, identifying unique characteristics of those outliers is a useful starting point.\r\rNext steps for further exploration\rThere is so much more to explore about the hospice system using this data set as a startin point. Some initial ideas to include:\n\rAnalyze average age as a variable\rCreate regression analyses of within each variable grouping to find the stongest predictors.\r\r\r\r\r","date":1576281600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576281600,"objectID":"a26a5a3f96f3fec17299b8b5b22a35fd","permalink":"/post/hospice/an-exploration-of-hospice-care-expenses-in-the-us/","publishdate":"2019-12-14T00:00:00Z","relpermalink":"/post/hospice/an-exploration-of-hospice-care-expenses-in-the-us/","section":"post","summary":"This project analyzes hospice care expenses throughout the United States and showcases a variety of R packages, data wrangling approaches, and statistical techniques. This was my final project for the *GSMDS5001: Fundamentals of Data Science with R* course.","tags":["Healthcare"],"title":"An Exploration of Hospice Care Expenses in the US","type":"post"},{"authors":[],"categories":["MBA"],"content":"\r\r\rAdditive Decomposition\rMultiplicative Decomposition\rForecasting 2002 - 2003\r\r\rDuring my MBA program I had the opportunity to take a course on busines and economic forecasting. The revenue cycle of many businesses can be highly seasonal, which adds a degree of complexity to creating forecasts. This is especially true in the retail sector, which made forecasting Target Corporation’s quarterly revenue from 1993-2001 a fun challenge!\n#setup\rlibrary(tidyverse)\rlibrary(fpp2)\rlibrary(kableExtra)\rlibrary(ggthemes)\rTargetLabF \u0026lt;- read_csv(\u0026quot;C:/Users/Lenovo/Desktop/AGSM/Fall2019/Forecasting/TargetDecomp/TargetLabF.csv\u0026quot;)\rTargetLabF$Period \u0026lt;- with(TargetLabF, paste(Yr, Quarter, sep = \u0026quot;:Q\u0026quot;))\r# Create Timeseries Data for Decomposition Analysis\rTargetTS \u0026lt;- TargetLabF %\u0026gt;%\rfilter(!is.na(Sales)) %\u0026gt;%\rselect(Time, Sales) %\u0026gt;%\rts(., start = c(1993, 1), end = c(2001, 4), frequency = 4)\r# Plot Data\rTargetLabF %\u0026gt;%\rfilter(!is.na(Sales)) %\u0026gt;%\rggplot(aes(x=Time, y=Sales)) + geom_line(size = 2) + scale_x_continuous(breaks = TargetLabF$Time, labels = TargetLabF$Period) +\rtheme_economist_white() +\rtheme(axis.text.x = element_text(angle = 90, hjust = 1)) +\rggtitle(\u0026quot;Target Quarterly Sales 1993:Q1 - 2001:Q4\u0026quot;) +\rylab(\u0026quot;Sales (in millions of US Dollars)\u0026quot;)\rTarget’s quarterly sales show a general upward trend with strong seasonality. There is a strong increase in Quarter 4 in each year, which is typical for the retail industry given the surge in holiday shopping. There isn’t a clear cycle, though, meaning that the trend is fairly constnat.\nSince the seasonal component here is constant from year to year, a classical decomposition method should work nicely here. We will work first through an additive decomposition and then through a multiplicative decomposition process.\nAdditive Decomposition\rTargetAddDecompm \u0026lt;- TargetTS %\u0026gt;% decompose(type = \u0026quot;additive\u0026quot;)\rTargetAddDecompm %\u0026gt;%\rautoplot() +\rggtitle(\u0026quot;Classical Additive Decomposition of Target Sales\u0026quot;) +\rxlab(\u0026quot;Year\u0026quot;) +\rtheme_economist_white() # choose economist theme\rThis plot is reasonably interesting, as it shows th trend and seasonally adjusted data alonogside each other, along with the error from the decomposition model.\n# Extract Seasonal and Trend Components\rSeasonal \u0026lt;- TargetAddDecompm$seasonal # Extract seasonal indecies SeasAdj \u0026lt;- (TargetTS/Seasonal) # Seasonally adjusted data TimeF \u0026lt;- TargetLabF %\u0026gt;% select(Time) # Extract TimeF\rTrendMod \u0026lt;- lm(SeasAdj ~ TargetTS) # Fit trend to seasonally adjusted data TimeFramef \u0026lt;- TargetLabF %\u0026gt;% select(Sales) # Extract sales\rTrend \u0026lt;- predict(TrendMod, # Project trend using trend model newdata = TargetTS)\rTrendDecomp \u0026lt;- TargetAddDecompm$trend #Extract trend value\rTrendDecompDF \u0026lt;- TrendDecomp[,2] %\u0026gt;% as.data.frame() # Convert trend value to df\rnames(TrendDecompDF) \u0026lt;- c(\u0026quot;Trend\u0026quot;) # rename\r# to find model coeficients\rsummary(TrendMod)\r## Response TargetTS.Time :\r## ## Call:\r## lm(formula = TargetTS.Time ~ TargetTS)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -0.043735 -0.017572 -0.003055 0.022056 0.041730 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -1.207e-01 1.294e-02 -9.326 9.03e-11 ***\r## TargetTSTime -1.168e-02 8.534e-04 -13.690 3.67e-15 ***\r## TargetTSSales 5.398e-05 4.450e-06 12.129 1.05e-13 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 0.02545 on 33 degrees of freedom\r## Multiple R-squared: 0.8503, Adjusted R-squared: 0.8413 ## F-statistic: 93.74 on 2 and 33 DF, p-value: 2.455e-14\r## ## ## Response TargetTS.Sales :\r## ## Call:\r## lm(formula = TargetTS.Sales ~ TargetTS)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -10.084 -4.671 -2.064 5.803 11.019 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -37.309120 3.250817 -11.48 4.61e-13 ***\r## TargetTSTime -2.650954 0.214332 -12.37 6.14e-14 ***\r## TargetTSSales 0.013913 0.001118 12.45 5.16e-14 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 6.392 on 33 degrees of freedom\r## Multiple R-squared: 0.8325, Adjusted R-squared: 0.8223 ## F-statistic: 81.99 on 2 and 33 DF, p-value: 1.575e-13\rTrendVals \u0026lt;- Trend %\u0026gt;% as.data.frame() %\u0026gt;% select(TargetTS.Sales) %\u0026gt;% ts(., start = c(1993, 1), end = c(2001, 4), frequency = 4)\r#create aggregate dataframe for plotting and performance anlaysis\rForecast \u0026lt;- TrendVals * Seasonal\rSalesData \u0026lt;- TargetTS %\u0026gt;% as.data.frame() %\u0026gt;% select(Sales) TimeData \u0026lt;- TargetTS %\u0026gt;% as.data.frame() %\u0026gt;% select(Time)\rAddPlot \u0026lt;- data.frame(TrendDecompDF, Forecast, SalesData, TimeData)\rnames(AddPlot) \u0026lt;- c(\u0026quot;Trend\u0026quot;, \u0026quot;Forecast\u0026quot;, \u0026quot;Data\u0026quot;, \u0026quot;Time\u0026quot;)\rAddPlot %\u0026gt;%\rgather(key=\u0026quot;DataType\u0026quot;, value=\u0026quot;Value\u0026quot;, 1:3) %\u0026gt;%\rggplot(aes(x=Time, y=Value, color = DataType)) + geom_line(size=1.5) + theme_economist_white() +\rscale_x_continuous(breaks = TargetLabF$Time, labels = TargetLabF$Period) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\rggtitle(\u0026quot;Additve Decomposition\u0026quot;, subtitle = \u0026quot;Forecast and Actual Data\u0026quot;) +\rylab(\u0026quot;Sales (in millions of US Dollars)\u0026quot;)\rSeasonal Indecies (in millions of dollars)\nSeasonal %\u0026gt;%\ras.tibble() %\u0026gt;%\rrowid_to_column() %\u0026gt;%\rfilter(rowid \u0026lt; 5) %\u0026gt;%\rrename(\u0026quot;Quarter\u0026quot; = \u0026quot;rowid\u0026quot;) %\u0026gt;%\rrename(\u0026quot;Seasonal_Indicator\u0026quot; = \u0026quot;x\u0026quot;)\r\r\r\rQuarter\r\rSeasonal_Indicator\r\r\r\r\r\r1\r\r-319.0977\r\r\r\r2\r\r-200.3242\r\r\r\r3\r\r-205.1211\r\r\r\r4\r\r724.5430\r\r\r\r\r\rPerformance Metrics\n# create all of the relevant performance metrics\rSSTO \u0026lt;- sum((AddPlot$Data - mean(AddPlot$Data))^2)\rSSE \u0026lt;- sum((AddPlot$Data - AddPlot$Forecast)^2)\rMSD \u0026lt;- SSE/length(AddPlot$Data)\rs \u0026lt;- sqrt(MSD)\rR2 \u0026lt;- (SSTO - SSE)/SSTO\rMAD \u0026lt;- sum(abs(AddPlot$Data - AddPlot$Forecast))/length(AddPlot$Data)\rMPE \u0026lt;- 100*sum((AddPlot$Data - AddPlot$Forecast)/AddPlot$Data)/length(AddPlot$Data)\rMAPE \u0026lt;- 100*sum(abs((AddPlot$Data - AddPlot$Forecast)/AddPlot$Data))/length(AddPlot$Data) Performance \u0026lt;- tibble(SSTO, SSE, MSD, s, R2, MAD, MPE, MAPE)\rAdditive Decomposition Performance Metrics\rPerformance %\u0026gt;% kable() %\u0026gt;% kable_styling()\r\r\rSSTO\r\rSSE\r\rMSD\r\rs\r\rR2\r\rMAD\r\rMPE\r\rMAPE\r\r\r\r\r\r142871385\r\r167622658\r\r4656185\r\r2157.819\r\r-0.1732416\r\r1743.858\r\r3.88285\r\r33.83439\r\r\r\r\r\rAdditive Decomposition Assessment\rThrough all of that, our predicted spike in quarter four is expected to continue, but the ability of the forecast to predict actual data in quater 3 in years 1996-1998 are far off, as the plot of forecast would suggest.\nWe have a farily low R2, which is the portion of variation explained by the model. This coincides with a high MAD (Mean Absolute Deviation) and SSE (Sum of Squered Error), which are both metrics of by how much the model misses actual data. While these are useful quantitative metircs, the of forecast against actual data indicates that perhaps we should try multiplicative decomposition methods for forecasting Target’s quarterly revenue.\n\r\rMultiplicative Decomposition\rTargetMultDecomp \u0026lt;- TargetTS %\u0026gt;% decompose(type = \u0026quot;multiplicative\u0026quot;) TargetMultDecomp %\u0026gt;%\rautoplot() +\rggtitle(\u0026quot;Classical Multiplicative Decomposition of Target Sales\u0026quot;) +\rxlab(\u0026quot;Year\u0026quot;) + theme_economist_white()\r# Extract Seasonal and Trend Components\rSeasonal \u0026lt;- TargetMultDecomp$seasonal SeasAdj \u0026lt;- (TargetTS/Seasonal) TimeF \u0026lt;- TargetLabF %\u0026gt;% select(Time)\rTrendMod \u0026lt;- lm(SeasAdj ~ TargetTS)\rTimeFramef \u0026lt;- TargetLabF %\u0026gt;% select(Sales)\rTrend \u0026lt;- predict(TrendMod, newdata = TargetTS)\rTrendDecomp \u0026lt;- TargetMultDecomp$trend\rTrendDecompDF \u0026lt;- TrendDecomp[,2] %\u0026gt;% as.data.frame()\rnames(TrendDecompDF) \u0026lt;- c(\u0026quot;Trend\u0026quot;)\rTrendVals \u0026lt;- Trend %\u0026gt;% as.data.frame() %\u0026gt;% select(TargetTS.Sales) %\u0026gt;% ts(., start = c(1993, 1), end = c(2001, 4), frequency = 4)\r#create aggregate dataframe for plotting and performance anlaysis\rForecast \u0026lt;- TrendVals * Seasonal\rSalesData \u0026lt;- TargetTS %\u0026gt;% as.data.frame() %\u0026gt;% select(Sales) TimeData \u0026lt;- TargetTS %\u0026gt;% as.data.frame() %\u0026gt;% select(Time)\rMultPlot \u0026lt;- data.frame(TrendDecompDF, Forecast, SalesData, TimeData)\rnames(MultPlot) \u0026lt;- c(\u0026quot;Trend\u0026quot;, \u0026quot;Forecast\u0026quot;, \u0026quot;Data\u0026quot;, \u0026quot;Time\u0026quot;)\rMultPlot %\u0026gt;%\rgather(key=\u0026quot;DataType\u0026quot;, value=\u0026quot;Value\u0026quot;, 1:3) %\u0026gt;%\rggplot(aes(x=Time, y=Value, color = DataType)) + geom_line(size=1.5) + scale_x_continuous(breaks = TargetLabF$Time, labels = TargetLabF$Period) +\rtheme_economist_white() +\rtheme(axis.text.x = element_text(angle = 90, hjust = 1)) +\rggtitle(\u0026quot;Multiplicative Decomposition\u0026quot;) +\rylab(\u0026quot;Sales (in millions of US Dollars)\u0026quot;)\rSeasonal %\u0026gt;%\ras.tibble() %\u0026gt;%\rrowid_to_column() %\u0026gt;%\rfilter(rowid \u0026lt; 5) %\u0026gt;%\rrename(\u0026quot;Quarter\u0026quot; = \u0026quot;rowid\u0026quot;) %\u0026gt;%\rrename(\u0026quot;Seasonal_Indicator\u0026quot; = \u0026quot;x\u0026quot;)\r\r\r\rQuarter\r\rSeasonal_Indicator\r\r\r\r\r\r1\r\r0.9387531\r\r\r\r2\r\r0.9632068\r\r\r\r3\r\r0.9580566\r\r\r\r4\r\r1.1399835\r\r\r\r\r\rSSTO \u0026lt;- sum((MultPlot$Data - mean(MultPlot$Data))^2)\rSSE \u0026lt;- sum((MultPlot$Data - MultPlot$Forecast)^2)\rMSD \u0026lt;- SSE/length(MultPlot$Data)\rs \u0026lt;- sqrt(MSD)\rR2 \u0026lt;- (SSTO - SSE)/SSTO\rMAD \u0026lt;- sum(abs(MultPlot$Data - MultPlot$Forecast))/length(MultPlot$Data)\rMPE \u0026lt;- 100*sum((MultPlot$Data - MultPlot$Forecast)/MultPlot$Data)/length(MultPlot$Data)\rMAPE \u0026lt;- 100*sum(abs((MultPlot$Data - MultPlot$Forecast)/MultPlot$Data))/length(MultPlot$Data) Performance \u0026lt;- tibble(SSTO, SSE, MSD, s, R2, MAD, MPE, MAPE)\rMultiplicative Decomposition Performance Metrics\rPerformance %\u0026gt;% kable() %\u0026gt;% kable_styling()\r\r\rSSTO\r\rSSE\r\rMSD\r\rs\r\rR2\r\rMAD\r\rMPE\r\rMAPE\r\r\r\r\r\r142871385\r\r283144.5\r\r7865.124\r\r88.68553\r\r0.9980182\r\r69.91029\r\r0.1328121\r\r1.487155\r\r\r\r\r\rMultiplicative Decomposition Assessment\rThe multiplicative model performs noteably better than the additive model. The R2 indicates that almost 99% of the varaition is captured by the model. As demonstrated by the low MAD and SSE values, the model has a fairly low total error. This model may actually be useful for forecasting Target’s 2002 and 2003 quarterly revenue. Let’s do it!\n\r\rForecasting 2002 - 2003\rTargetTS %\u0026gt;% forecast() %\u0026gt;%\rautoplot() +\rggtitle(\u0026quot;Target Sales Forecast Using Multiplicative Decomposition\u0026quot;) + ylab(\u0026quot;Sales \u0026amp; Forecast (in millions of US dollars\u0026quot;) +\rtheme_economist_white()\rTargetTS %\u0026gt;% forecast() %\u0026gt;%\ras.data.frame() %\u0026gt;% filter(Series == \u0026quot;Sales\u0026quot;) %\u0026gt;%\rselect(-Series)\r\r\r\rTime\r\rPoint Forecast\r\rLo 80\r\rHi 80\r\rLo 95\r\rHi 95\r\r\r\r\r\r2002 Q1\r\r7488.757\r\r7288.633\r\r7688.881\r\r7182.693\r\r7794.821\r\r\r\r2002 Q2\r\r8084.943\r\r7838.603\r\r8331.283\r\r7708.198\r\r8461.687\r\r\r\r2002 Q3\r\r8201.530\r\r7918.507\r\r8484.552\r\r7768.684\r\r8634.375\r\r\r\r2002 Q4\r\r11688.782\r\r11235.422\r\r12142.141\r\r10995.428\r\r12382.135\r\r\r\r2003 Q1\r\r8178.980\r\r7825.157\r\r8532.802\r\r7637.855\r\r8720.105\r\r\r\r2003 Q2\r\r8813.331\r\r8391.215\r\r9235.447\r\r8167.761\r\r9458.902\r\r\r\r2003 Q3\r\r8924.146\r\r8454.131\r\r9394.161\r\r8205.321\r\r9642.972\r\r\r\r2003 Q4\r\r12696.455\r\r11965.674\r\r13427.237\r\r11578.821\r\r13814.089\r\r\r\r\r\r\r","date":1572480000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590703822,"objectID":"04d35794891ab6ba56263c57680f55d4","permalink":"/post/target-quarterly-revenue-forecasting/","publishdate":"2019-10-31T00:00:00Z","relpermalink":"/post/target-quarterly-revenue-forecasting/","section":"post","summary":"This post goes through a few forecasting methods to analyze and predict quarterly revenue for the Target Corporation. This post is a version of a project I worked on during my MBA program with Roxanne Adams and Matt Stone.","tags":["Forecasting","Finance"],"title":"Forecasting Target Corporation's Quarterly Revenue","type":"post"},{"authors":["Zachary Dyne"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["Zachary Dyne","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Zachary Dyne","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"}]